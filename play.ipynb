{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import datetime\n",
    "import openai\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# OpenAI API key (replace with your actual key)\n",
    "openai.api_key = \"your-openai-api-key\"\n",
    "\n",
    "def fetch_papers() -> List[arxiv.Result]:\n",
    "    \"\"\"Fetch papers from arXiv for the cs.CL category published today.\"\"\"\n",
    "    today = datetime.date.today()\n",
    "    query = f\"cat:cs.CL AND submittedDate:[{today}T00:00:00Z TO {today}T23:59:59Z]\"\n",
    "    \n",
    "    client = arxiv.Client(\n",
    "        page_size=100,\n",
    "        delay_seconds=3.0,\n",
    "        num_retries=3\n",
    "    )\n",
    "    client.results(search=arxiv.Search(query=query, max_results=100))\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=100,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    papers = list(c.results())\n",
    "    logging.info(f\"Fetched {len(papers)} papers from arXiv\")\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qh/dh3wd_9j78q_zgx9v9lxwzg40000gq/T/ipykernel_41445/980543543.py:24: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  papers = list(search.results())\n",
      "2024-07-01 14:52:35,241 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=cat%3Acs.CL+AND+submittedDate%3A%5B2024-07-01T00%3A00%3A00Z+TO+2024-07-01T23%3A59%3A59Z%5D&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100\n",
      "2024-07-01 14:52:36,465 - INFO - Got empty first page; stopping generation\n",
      "2024-07-01 14:52:36,466 - INFO - Fetched 0 papers from arXiv\n"
     ]
    }
   ],
   "source": [
    "a = fetch_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = arxiv.Client(\n",
    "    page_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = client.results(\n",
    "    search=arxiv.Search(\n",
    "        query=\"cat:cs.CL\",\n",
    "        max_results=5,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 00:49:25,053 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=cat%3Acs.CL&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=10\n",
      "2024-07-09 00:49:26,384 - INFO - Got first page: 10 of 65762 total results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs\n",
      "Authors: Rudolf Laine, Bilal Chughtai, Jan Betley, Kaivalya Hariharan, Jeremy Scheurer, Mikita Balesni, Marius Hobbhahn, Alexander Meinke, Owain Evans\n",
      "Published: 2024-07-05 17:57:02+00:00\n",
      "DOI URL: http://arxiv.org/pdf/2407.04694v1\n",
      "Title: ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models\n",
      "Authors: Yuzhe Gu, Ziwei Ji, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen\n",
      "Published: 2024-07-05 17:56:38+00:00\n",
      "DOI URL: http://arxiv.org/pdf/2407.04693v1\n",
      "Title: Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for Interpreting Neural Networks\n",
      "Authors: Aaron Mueller\n",
      "Published: 2024-07-05 17:53:03+00:00\n",
      "DOI URL: http://arxiv.org/pdf/2407.04690v1\n",
      "Title: Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge\n",
      "Authors: Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip Torr, Lu Yuan\n",
      "Published: 2024-07-05 17:43:30+00:00\n",
      "DOI URL: http://arxiv.org/pdf/2407.04681v1\n",
      "Title: Lost in Translation: The Algorithmic Gap Between LMs and the Brain\n",
      "Authors: Tommaso Tosato, Pascal Jr Tikeng Notsawo, Saskia Helbling, Irina Rish, Guillaume Dumas\n",
      "Published: 2024-07-05 17:43:16+00:00\n",
      "DOI URL: http://arxiv.org/pdf/2407.04680v1\n"
     ]
    }
   ],
   "source": [
    "for result in aa:\n",
    "    print(f\"Title: {result.title}\")\n",
    "    print(f\"Authors: {', '.join(author.name for author in result.authors)}\")\n",
    "    print(f\"Published: {result.published}\")\n",
    "    print(f\"DOI URL: {result.pdf_url}\")\n",
    "    # print(f\"Summary: {result.summary}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qh/dh3wd_9j78q_zgx9v9lxwzg40000gq/T/ipykernel_7918/1329716450.py:18: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages\n",
      "Authors: Max Zuo, Francisco Piedrahita Velez, Xiaochen Li, Michael L. Littman, Stephen H. Bach\n",
      "Published: 2024-07-03 17:59:53+00:00\n",
      "Summary: Many recent works have explored using language models for planning problems.\n",
      "One line of research focuses on translating natural language descriptions of\n",
      "planning tasks into structured planning languages, such as the planning domain\n",
      "definition language (PDDL). While this approach is promising, accurately\n",
      "measuring the quality of generated PDDL code continues to pose significant\n",
      "challenges. First, generated PDDL code is typically evaluated using planning\n",
      "validators that check whether the problem can be solved with a planner. This\n",
      "method is insufficient because a language model might generate valid PDDL code\n",
      "that does not align with the natural language description of the task. Second,\n",
      "existing evaluation sets often have natural language descriptions of the\n",
      "planning task that closely resemble the ground truth PDDL, reducing the\n",
      "challenge of the task. To bridge this gap, we introduce \\benchmarkName, a\n",
      "benchmark designed to evaluate language models' ability to generate PDDL code\n",
      "from natural language descriptions of planning tasks. We begin by creating a\n",
      "PDDL equivalence algorithm that rigorously evaluates the correctness of PDDL\n",
      "code generated by language models by flexibly comparing it against a ground\n",
      "truth PDDL. Then, we present a dataset of $132,037$ text-to-PDDL pairs across\n",
      "13 different tasks, with varying levels of difficulty. Finally, we evaluate\n",
      "several API-access and open-weight language models that reveal this task's\n",
      "complexity. For example, $87.6\\%$ of the PDDL problem descriptions generated by\n",
      "GPT-4o are syntactically parseable, $82.2\\%$ are valid, solve-able problems,\n",
      "but only $35.1\\%$ are semantically correct, highlighting the need for a more\n",
      "rigorous benchmark for this problem.\n",
      "\n",
      "Title: InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output\n",
      "Authors: Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang\n",
      "Published: 2024-07-03 17:59:21+00:00\n",
      "Summary: We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\n",
      "language model that supports long-contextual input and output. IXC-2.5 excels\n",
      "in various text-image comprehension and composition applications, achieving\n",
      "GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\n",
      "interleaved image-text contexts, it can seamlessly extend to 96K long contexts\n",
      "via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\n",
      "tasks requiring extensive input and output contexts. Compared to its previous\n",
      "2.0 version, InternLM-XComposer-2.5 features three major upgrades in\n",
      "vision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\n",
      "Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\n",
      "addition to comprehension, IXC-2.5 extends to two compelling applications using\n",
      "extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\n",
      "Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\n",
      "benchmarks, outperforming existing open-source state-of-the-art models on 16\n",
      "benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n",
      "16 key tasks. The InternLM-XComposer-2.5 is publicly available at\n",
      "https://github.com/InternLM/InternLM-XComposer.\n",
      "\n",
      "Title: BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations\n",
      "Authors: Zhantao Yang, Ruili Feng, Keyu Yan, Huangji Wang, Zhicai Wang, Shangwen Zhu, Han Zhang, Jie Xiao, Pingyu Wu, Kai Zhu, Jixuan Chen, Chen-Wei Xie, Chaojie Mao, Yue Yang, Hongyang Zhang, Yu Liu, Fan Cheng\n",
      "Published: 2024-07-03 17:55:27+00:00\n",
      "Summary: This paper presents Bag-of-Concept Graph (BACON) to gift models with limited\n",
      "linguistic abilities to taste the privilege of Vision Language Models (VLMs)\n",
      "and boost downstream tasks such as detection, visual question answering (VQA),\n",
      "and image generation. Since the visual scenes in physical worlds are structured\n",
      "with complex relations between objects, BACON breaks down annotations into\n",
      "basic minimum elements and presents them in a graph structure. Element-wise\n",
      "style enables easy understanding, and structural composition liberates\n",
      "difficult locating. Careful prompt design births the BACON captions with the\n",
      "help of public-available VLMs and segmentation methods. In this way, we gather\n",
      "a dataset with 100K annotated images, which endow VLMs with remarkable\n",
      "capabilities, such as accurately generating BACON, transforming prompts into\n",
      "BACON format, envisioning scenarios in the style of BACONr, and dynamically\n",
      "modifying elements within BACON through interactive dialogue and more. Wide\n",
      "representative experiments, including detection, VQA, and image generation\n",
      "tasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel\n",
      "in their current cutting-edge solutions.\n",
      "\n",
      "Title: A Review of the Applications of Deep Learning-Based Emergent Communication\n",
      "Authors: Brendon Boldt, David Mortensen\n",
      "Published: 2024-07-03 17:43:54+00:00\n",
      "Summary: Emergent communication, or emergent language, is the field of research which\n",
      "studies how human language-like communication systems emerge de novo in deep\n",
      "multi-agent reinforcement learning environments. The possibilities of\n",
      "replicating the emergence of a complex behavior like language have strong\n",
      "intuitive appeal, yet it is necessary to complement this with clear notions of\n",
      "how such research can be applicable to other fields of science, technology, and\n",
      "engineering. This paper comprehensively reviews the applications of emergent\n",
      "communication research across machine learning, natural language processing,\n",
      "linguistics, and cognitive science. Each application is illustrated with a\n",
      "description of its scope, an explication of emergent communication's unique\n",
      "role in addressing it, a summary of the extant literature working towards the\n",
      "application, and brief recommendations for near-term research directions.\n",
      "\n",
      "Title: LLM Internal States Reveal Hallucination Risk Faced With a Query\n",
      "Authors: Ziwei Ji, Delong Chen, Etsuko Ishii, Samuel Cahyawijaya, Yejin Bang, Bryan Wilie, Pascale Fung\n",
      "Published: 2024-07-03 17:08:52+00:00\n",
      "Summary: The hallucination problem of Large Language Models (LLMs) significantly\n",
      "limits their reliability and trustworthiness. Humans have a self-awareness\n",
      "process that allows us to recognize what we don't know when faced with queries.\n",
      "Inspired by this, our paper investigates whether LLMs can estimate their own\n",
      "hallucination risk before response generation. We analyze the internal\n",
      "mechanisms of LLMs broadly both in terms of training data sources and across 15\n",
      "diverse Natural Language Generation (NLG) tasks, spanning over 700 datasets.\n",
      "Our empirical analysis reveals two key insights: (1) LLM internal states\n",
      "indicate whether they have seen the query in training data or not; and (2) LLM\n",
      "internal states show they are likely to hallucinate or not regarding the query.\n",
      "Our study explores particular neurons, activation layers, and tokens that play\n",
      "a crucial role in the LLM perception of uncertainty and hallucination risk. By\n",
      "a probing estimator, we leverage LLM self-assessment, achieving an average\n",
      "hallucination estimation accuracy of 84.32\\% at run time.\n",
      "\n",
      "Title: Evaluating Automatic Metrics with Incremental Machine Translation Systems\n",
      "Authors: Guojun Wu, Shay B. Cohen, Rico Sennrich\n",
      "Published: 2024-07-03 17:04:17+00:00\n",
      "Summary: We introduce a dataset comprising commercial machine translations, gathered\n",
      "weekly over six years across 12 translation directions. Since human A/B testing\n",
      "is commonly used, we assume commercial systems improve over time, which enables\n",
      "us to evaluate machine translation (MT) metrics based on their preference for\n",
      "more recent translations. Our study confirms several previous findings in MT\n",
      "metrics research and demonstrates the dataset's value as a testbed for metric\n",
      "evaluation. We release our code at https://github.com/gjwubyron/Evo\n",
      "\n",
      "Title: How Similar Are Elected Politicians and Their Constituents? Quantitative Evidence From Online Social Network\n",
      "Authors: Waleed Iqbal, Gareth Tyson, Ignacio Castro\n",
      "Published: 2024-07-03 16:36:26+00:00\n",
      "Summary: How similar are politicians to those who vote for them? This is a critical\n",
      "question at the heart of democratic representation and particularly relevant at\n",
      "times when political dissatisfaction and populism are on the rise. To answer\n",
      "this question we compare the online discourse of elected politicians and their\n",
      "constituents. We collect a two and a half years (September 2020 - February\n",
      "2023) constituency-level dataset for USA and UK that includes: (i) the Twitter\n",
      "timelines (5.6 Million tweets) of elected political representatives (595 UK\n",
      "Members of Parliament and 433 USA Representatives), (ii) the Nextdoor posts\n",
      "(21.8 Million posts) of the constituency (98.4% USA and 91.5% UK\n",
      "constituencies). We find that elected politicians tend to be equally similar to\n",
      "their constituents in terms of content and style regardless of whether a\n",
      "constituency elects a right or left-wing politician. The size of the electoral\n",
      "victory and the level of income of a constituency shows a nuanced picture. The\n",
      "narrower the electoral victory, the more similar the style and the more\n",
      "dissimilar the content is. The lower the income of a constituency, the more\n",
      "similar the content is. In terms of style, poorer constituencies tend to have a\n",
      "more similar sentiment and more dissimilar psychological text traits (i.e.\n",
      "measured with LIWC categories).\n",
      "\n",
      "Title: STF: Sentence Transformer Fine-Tuning For Topic Categorization With Limited Data\n",
      "Authors: Kheir Eddine Daouadi, Yaakoub Boualleg, Oussama Guehairia\n",
      "Published: 2024-07-03 16:34:56+00:00\n",
      "Summary: Nowadays, topic classification from tweets attracts considerable research\n",
      "attention. Different classification systems have been suggested thanks to these\n",
      "research efforts. Nevertheless, they face major challenges owing to low\n",
      "performance metrics due to the limited amount of labeled data. We propose\n",
      "Sentence Transformers Fine-tuning (STF), a topic detection system that\n",
      "leverages pretrained Sentence Transformers models and fine-tuning to classify\n",
      "topics from tweets accurately. Moreover, extensive parameter sensitivity\n",
      "analyses were conducted to finetune STF parameters for our topic classification\n",
      "task to achieve the best performance results. Experiments on two benchmark\n",
      "datasets demonstrated that (1) the proposed STF can be effectively used for\n",
      "classifying tweet topics and outperforms the latest state-of-the-art\n",
      "approaches, and (2) the proposed STF does not require a huge amount of labeled\n",
      "tweets to achieve good accuracy, which is a limitation of many state-of-the-art\n",
      "approaches. Our main contribution is the achievement of promising results in\n",
      "tweet topic classification by applying pretrained sentence transformers\n",
      "language models.\n",
      "\n",
      "Title: CATT: Character-based Arabic Tashkeel Transformer\n",
      "Authors: Faris Alasmary, Orjuwan Zaafarani, Ahmad Ghannam\n",
      "Published: 2024-07-03 16:05:20+00:00\n",
      "Summary: Tashkeel, or Arabic Text Diacritization (ATD), greatly enhances the\n",
      "comprehension of Arabic text by removing ambiguity and minimizing the risk of\n",
      "misinterpretations caused by its absence. It plays a crucial role in improving\n",
      "Arabic text processing, particularly in applications such as text-to-speech and\n",
      "machine translation. This paper introduces a new approach to training ATD\n",
      "models. First, we finetuned two transformers, encoder-only and encoder-decoder,\n",
      "that were initialized from a pretrained character-based BERT. Then, we applied\n",
      "the Noisy-Student approach to boost the performance of the best model. We\n",
      "evaluated our models alongside 11 commercial and open-source models using two\n",
      "manually labeled benchmark datasets: WikiNews and our CATT dataset. Our\n",
      "findings show that our top model surpasses all evaluated models by relative\n",
      "Diacritic Error Rates (DERs) of 30.83\\% and 35.21\\% on WikiNews and CATT,\n",
      "respectively, achieving state-of-the-art in ATD. In addition, we show that our\n",
      "model outperforms GPT-4-turbo on CATT dataset by a relative DER of 9.36\\%. We\n",
      "open-source our CATT models and benchmark dataset for the research\n",
      "community\\footnote{https://github.com/abjadai/catt}.\n",
      "\n",
      "Title: Self-Evaluation as a Defense Against Adversarial Attacks on LLMs\n",
      "Authors: Hannah Brown, Leon Lin, Kenji Kawaguchi, Michael Shieh\n",
      "Published: 2024-07-03 16:03:42+00:00\n",
      "Summary: When LLMs are deployed in sensitive, human-facing settings, it is crucial\n",
      "that they do not output unsafe, biased, or privacy-violating outputs. For this\n",
      "reason, models are both trained and instructed to refuse to answer unsafe\n",
      "prompts such as \"Tell me how to build a bomb.\" We find that, despite these\n",
      "safeguards, it is possible to break model defenses simply by appending a space\n",
      "to the end of a model's input. In a study of eight open-source models, we\n",
      "demonstrate that this acts as a strong enough attack to cause the majority of\n",
      "models to generate harmful outputs with very high success rates. We examine the\n",
      "causes of this behavior, finding that the contexts in which single spaces occur\n",
      "in tokenized training data encourage models to generate lists when prompted,\n",
      "overriding training signals to refuse to answer unsafe requests. Our findings\n",
      "underscore the fragile state of current model alignment and promote the\n",
      "importance of developing more robust alignment methods. Code and data will be\n",
      "made available at https://github.com/Linlt-leon/Adversarial-Alignments.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define search parameters\n",
    "search_query = \"cat:cs.CL\"  # Computation and Language category\n",
    "today = datetime.now()\n",
    "start_date = today.strftime(\"%Y%m%d%H%M%S\")\n",
    "end_date = (today + timedelta(days=1)).strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Search for papers\n",
    "search = arxiv.Search(\n",
    "    query=search_query,\n",
    "    max_results=10,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    ")\n",
    "\n",
    "# Fetch and display results\n",
    "for result in search.results():\n",
    "    print(f\"Title: {result.title}\")\n",
    "    print(f\"Authors: {', '.join(author.name for author in result.authors)}\")\n",
    "    print(f\"Published: {result.published}\")\n",
    "    print(f\"Summary: {result.summary}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qh/dh3wd_9j78q_zgx9v9lxwzg40000gq/T/ipykernel_7918/4229642230.py:1: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  list(search.results())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[arxiv.Result(entry_id='http://arxiv.org/abs/2407.03321v1', updated=datetime.datetime(2024, 7, 3, 17, 59, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 3, 17, 59, 53, tzinfo=datetime.timezone.utc), title='Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages', authors=[arxiv.Result.Author('Max Zuo'), arxiv.Result.Author('Francisco Piedrahita Velez'), arxiv.Result.Author('Xiaochen Li'), arxiv.Result.Author('Michael L. Littman'), arxiv.Result.Author('Stephen H. Bach')], summary=\"Many recent works have explored using language models for planning problems.\\nOne line of research focuses on translating natural language descriptions of\\nplanning tasks into structured planning languages, such as the planning domain\\ndefinition language (PDDL). While this approach is promising, accurately\\nmeasuring the quality of generated PDDL code continues to pose significant\\nchallenges. First, generated PDDL code is typically evaluated using planning\\nvalidators that check whether the problem can be solved with a planner. This\\nmethod is insufficient because a language model might generate valid PDDL code\\nthat does not align with the natural language description of the task. Second,\\nexisting evaluation sets often have natural language descriptions of the\\nplanning task that closely resemble the ground truth PDDL, reducing the\\nchallenge of the task. To bridge this gap, we introduce \\\\benchmarkName, a\\nbenchmark designed to evaluate language models' ability to generate PDDL code\\nfrom natural language descriptions of planning tasks. We begin by creating a\\nPDDL equivalence algorithm that rigorously evaluates the correctness of PDDL\\ncode generated by language models by flexibly comparing it against a ground\\ntruth PDDL. Then, we present a dataset of $132,037$ text-to-PDDL pairs across\\n13 different tasks, with varying levels of difficulty. Finally, we evaluate\\nseveral API-access and open-weight language models that reveal this task's\\ncomplexity. For example, $87.6\\\\%$ of the PDDL problem descriptions generated by\\nGPT-4o are syntactically parseable, $82.2\\\\%$ are valid, solve-able problems,\\nbut only $35.1\\\\%$ are semantically correct, highlighting the need for a more\\nrigorous benchmark for this problem.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.03321v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.03321v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2407.03320v1', updated=datetime.datetime(2024, 7, 3, 17, 59, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 3, 17, 59, 21, tzinfo=datetime.timezone.utc), title='InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output', authors=[arxiv.Result.Author('Pan Zhang'), arxiv.Result.Author('Xiaoyi Dong'), arxiv.Result.Author('Yuhang Zang'), arxiv.Result.Author('Yuhang Cao'), arxiv.Result.Author('Rui Qian'), arxiv.Result.Author('Lin Chen'), arxiv.Result.Author('Qipeng Guo'), arxiv.Result.Author('Haodong Duan'), arxiv.Result.Author('Bin Wang'), arxiv.Result.Author('Linke Ouyang'), arxiv.Result.Author('Songyang Zhang'), arxiv.Result.Author('Wenwei Zhang'), arxiv.Result.Author('Yining Li'), arxiv.Result.Author('Yang Gao'), arxiv.Result.Author('Peng Sun'), arxiv.Result.Author('Xinyue Zhang'), arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Jingwen Li'), arxiv.Result.Author('Wenhai Wang'), arxiv.Result.Author('Hang Yan'), arxiv.Result.Author('Conghui He'), arxiv.Result.Author('Xingcheng Zhang'), arxiv.Result.Author('Kai Chen'), arxiv.Result.Author('Jifeng Dai'), arxiv.Result.Author('Yu Qiao'), arxiv.Result.Author('Dahua Lin'), arxiv.Result.Author('Jiaqi Wang')], summary='We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\\nin various text-image comprehension and composition applications, achieving\\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\\ntasks requiring extensive input and output contexts. Compared to its previous\\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\\naddition to comprehension, IXC-2.5 extends to two compelling applications using\\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\\nhttps://github.com/InternLM/InternLM-XComposer.', comment='Technical Report. https://github.com/InternLM/InternLM-XComposer', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.03320v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.03320v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2407.03314v1', updated=datetime.datetime(2024, 7, 3, 17, 55, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 3, 17, 55, 27, tzinfo=datetime.timezone.utc), title='BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations', authors=[arxiv.Result.Author('Zhantao Yang'), arxiv.Result.Author('Ruili Feng'), arxiv.Result.Author('Keyu Yan'), arxiv.Result.Author('Huangji Wang'), arxiv.Result.Author('Zhicai Wang'), arxiv.Result.Author('Shangwen Zhu'), arxiv.Result.Author('Han Zhang'), arxiv.Result.Author('Jie Xiao'), arxiv.Result.Author('Pingyu Wu'), arxiv.Result.Author('Kai Zhu'), arxiv.Result.Author('Jixuan Chen'), arxiv.Result.Author('Chen-Wei Xie'), arxiv.Result.Author('Chaojie Mao'), arxiv.Result.Author('Yue Yang'), arxiv.Result.Author('Hongyang Zhang'), arxiv.Result.Author('Yu Liu'), arxiv.Result.Author('Fan Cheng')], summary='This paper presents Bag-of-Concept Graph (BACON) to gift models with limited\\nlinguistic abilities to taste the privilege of Vision Language Models (VLMs)\\nand boost downstream tasks such as detection, visual question answering (VQA),\\nand image generation. Since the visual scenes in physical worlds are structured\\nwith complex relations between objects, BACON breaks down annotations into\\nbasic minimum elements and presents them in a graph structure. Element-wise\\nstyle enables easy understanding, and structural composition liberates\\ndifficult locating. Careful prompt design births the BACON captions with the\\nhelp of public-available VLMs and segmentation methods. In this way, we gather\\na dataset with 100K annotated images, which endow VLMs with remarkable\\ncapabilities, such as accurately generating BACON, transforming prompts into\\nBACON format, envisioning scenarios in the style of BACONr, and dynamically\\nmodifying elements within BACON through interactive dialogue and more. Wide\\nrepresentative experiments, including detection, VQA, and image generation\\ntasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel\\nin their current cutting-edge solutions.', comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'cs.CL', 'cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.03314v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.03314v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2407.03302v1', updated=datetime.datetime(2024, 7, 3, 17, 43, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 3, 17, 43, 54, tzinfo=datetime.timezone.utc), title='A Review of the Applications of Deep Learning-Based Emergent Communication', authors=[arxiv.Result.Author('Brendon Boldt'), arxiv.Result.Author('David Mortensen')], summary=\"Emergent communication, or emergent language, is the field of research which\\nstudies how human language-like communication systems emerge de novo in deep\\nmulti-agent reinforcement learning environments. The possibilities of\\nreplicating the emergence of a complex behavior like language have strong\\nintuitive appeal, yet it is necessary to complement this with clear notions of\\nhow such research can be applicable to other fields of science, technology, and\\nengineering. This paper comprehensively reviews the applications of emergent\\ncommunication research across machine learning, natural language processing,\\nlinguistics, and cognitive science. Each application is illustrated with a\\ndescription of its scope, an explication of emergent communication's unique\\nrole in addressing it, a summary of the extant literature working towards the\\napplication, and brief recommendations for near-term research directions.\", comment='49 pages, 15 figures', journal_ref='Transactions on Machine Learning Research, February 2024', doi=None, primary_category='cs.CL', categories=['cs.CL', 'I.2.7; I.6.m'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.03302v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.03302v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2407.03282v1', updated=datetime.datetime(2024, 7, 3, 17, 8, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 3, 17, 8, 52, tzinfo=datetime.timezone.utc), title='LLM Internal States Reveal Hallucination Risk Faced With a Query', authors=[arxiv.Result.Author('Ziwei Ji'), arxiv.Result.Author('Delong Chen'), arxiv.Result.Author('Etsuko Ishii'), arxiv.Result.Author('Samuel Cahyawijaya'), arxiv.Result.Author('Yejin Bang'), arxiv.Result.Author('Bryan Wilie'), arxiv.Result.Author('Pascale Fung')], summary=\"The hallucination problem of Large Language Models (LLMs) significantly\\nlimits their reliability and trustworthiness. Humans have a self-awareness\\nprocess that allows us to recognize what we don't know when faced with queries.\\nInspired by this, our paper investigates whether LLMs can estimate their own\\nhallucination risk before response generation. We analyze the internal\\nmechanisms of LLMs broadly both in terms of training data sources and across 15\\ndiverse Natural Language Generation (NLG) tasks, spanning over 700 datasets.\\nOur empirical analysis reveals two key insights: (1) LLM internal states\\nindicate whether they have seen the query in training data or not; and (2) LLM\\ninternal states show they are likely to hallucinate or not regarding the query.\\nOur study explores particular neurons, activation layers, and tokens that play\\na crucial role in the LLM perception of uncertainty and hallucination risk. By\\na probing estimator, we leverage LLM self-assessment, achieving an average\\nhallucination estimation accuracy of 84.32\\\\% at run time.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.03282v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.03282v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2407.03277v1', updated=datetime.datetime(2024, 7, 3, 17, 4, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 3, 17, 4, 17, tzinfo=datetime.timezone.utc), title='Evaluating Automatic Metrics with Incremental Machine Translation Systems', authors=[arxiv.Result.Author('Guojun Wu'), arxiv.Result.Author('Shay B. Cohen'), arxiv.Result.Author('Rico Sennrich')], summary=\"We introduce a dataset comprising commercial machine translations, gathered\\nweekly over six years across 12 translation directions. Since human A/B testing\\nis commonly used, we assume commercial systems improve over time, which enables\\nus to evaluate machine translation (MT) metrics based on their preference for\\nmore recent translations. Our study confirms several previous findings in MT\\nmetrics research and demonstrates the dataset's value as a testbed for metric\\nevaluation. We release our code at https://github.com/gjwubyron/Evo\", comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.03277v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.03277v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2407.03255v1', updated=datetime.datetime(2024, 7, 3, 16, 36, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 3, 16, 36, 26, tzinfo=datetime.timezone.utc), title='How Similar Are Elected Politicians and Their Constituents? Quantitative Evidence From Online Social Network', authors=[arxiv.Result.Author('Waleed Iqbal'), arxiv.Result.Author('Gareth Tyson'), arxiv.Result.Author('Ignacio Castro')], summary='How similar are politicians to those who vote for them? This is a critical\\nquestion at the heart of democratic representation and particularly relevant at\\ntimes when political dissatisfaction and populism are on the rise. To answer\\nthis question we compare the online discourse of elected politicians and their\\nconstituents. We collect a two and a half years (September 2020 - February\\n2023) constituency-level dataset for USA and UK that includes: (i) the Twitter\\ntimelines (5.6 Million tweets) of elected political representatives (595 UK\\nMembers of Parliament and 433 USA Representatives), (ii) the Nextdoor posts\\n(21.8 Million posts) of the constituency (98.4% USA and 91.5% UK\\nconstituencies). We find that elected politicians tend to be equally similar to\\ntheir constituents in terms of content and style regardless of whether a\\nconstituency elects a right or left-wing politician. The size of the electoral\\nvictory and the level of income of a constituency shows a nuanced picture. The\\nnarrower the electoral victory, the more similar the style and the more\\ndissimilar the content is. The lower the income of a constituency, the more\\nsimilar the content is. In terms of style, poorer constituencies tend to have a\\nmore similar sentiment and more dissimilar psychological text traits (i.e.\\nmeasured with LIWC categories).', comment=None, journal_ref=None, doi=None, primary_category='cs.SI', categories=['cs.SI', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.03255v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.03255v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2407.03253v1', updated=datetime.datetime(2024, 7, 3, 16, 34, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 3, 16, 34, 56, tzinfo=datetime.timezone.utc), title='STF: Sentence Transformer Fine-Tuning For Topic Categorization With Limited Data', authors=[arxiv.Result.Author('Kheir Eddine Daouadi'), arxiv.Result.Author('Yaakoub Boualleg'), arxiv.Result.Author('Oussama Guehairia')], summary='Nowadays, topic classification from tweets attracts considerable research\\nattention. Different classification systems have been suggested thanks to these\\nresearch efforts. Nevertheless, they face major challenges owing to low\\nperformance metrics due to the limited amount of labeled data. We propose\\nSentence Transformers Fine-tuning (STF), a topic detection system that\\nleverages pretrained Sentence Transformers models and fine-tuning to classify\\ntopics from tweets accurately. Moreover, extensive parameter sensitivity\\nanalyses were conducted to finetune STF parameters for our topic classification\\ntask to achieve the best performance results. Experiments on two benchmark\\ndatasets demonstrated that (1) the proposed STF can be effectively used for\\nclassifying tweet topics and outperforms the latest state-of-the-art\\napproaches, and (2) the proposed STF does not require a huge amount of labeled\\ntweets to achieve good accuracy, which is a limitation of many state-of-the-art\\napproaches. Our main contribution is the achievement of promising results in\\ntweet topic classification by applying pretrained sentence transformers\\nlanguage models.', comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.03253v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.03253v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2407.03236v1', updated=datetime.datetime(2024, 7, 3, 16, 5, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 3, 16, 5, 20, tzinfo=datetime.timezone.utc), title='CATT: Character-based Arabic Tashkeel Transformer', authors=[arxiv.Result.Author('Faris Alasmary'), arxiv.Result.Author('Orjuwan Zaafarani'), arxiv.Result.Author('Ahmad Ghannam')], summary='Tashkeel, or Arabic Text Diacritization (ATD), greatly enhances the\\ncomprehension of Arabic text by removing ambiguity and minimizing the risk of\\nmisinterpretations caused by its absence. It plays a crucial role in improving\\nArabic text processing, particularly in applications such as text-to-speech and\\nmachine translation. This paper introduces a new approach to training ATD\\nmodels. First, we finetuned two transformers, encoder-only and encoder-decoder,\\nthat were initialized from a pretrained character-based BERT. Then, we applied\\nthe Noisy-Student approach to boost the performance of the best model. We\\nevaluated our models alongside 11 commercial and open-source models using two\\nmanually labeled benchmark datasets: WikiNews and our CATT dataset. Our\\nfindings show that our top model surpasses all evaluated models by relative\\nDiacritic Error Rates (DERs) of 30.83\\\\% and 35.21\\\\% on WikiNews and CATT,\\nrespectively, achieving state-of-the-art in ATD. In addition, we show that our\\nmodel outperforms GPT-4-turbo on CATT dataset by a relative DER of 9.36\\\\%. We\\nopen-source our CATT models and benchmark dataset for the research\\ncommunity\\\\footnote{https://github.com/abjadai/catt}.', comment=None, journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.03236v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.03236v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2407.03234v1', updated=datetime.datetime(2024, 7, 3, 16, 3, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 7, 3, 16, 3, 42, tzinfo=datetime.timezone.utc), title='Self-Evaluation as a Defense Against Adversarial Attacks on LLMs', authors=[arxiv.Result.Author('Hannah Brown'), arxiv.Result.Author('Leon Lin'), arxiv.Result.Author('Kenji Kawaguchi'), arxiv.Result.Author('Michael Shieh')], summary='When LLMs are deployed in sensitive, human-facing settings, it is crucial\\nthat they do not output unsafe, biased, or privacy-violating outputs. For this\\nreason, models are both trained and instructed to refuse to answer unsafe\\nprompts such as \"Tell me how to build a bomb.\" We find that, despite these\\nsafeguards, it is possible to break model defenses simply by appending a space\\nto the end of a model\\'s input. In a study of eight open-source models, we\\ndemonstrate that this acts as a strong enough attack to cause the majority of\\nmodels to generate harmful outputs with very high success rates. We examine the\\ncauses of this behavior, finding that the contexts in which single spaces occur\\nin tokenized training data encourage models to generate lists when prompted,\\noverriding training signals to refuse to answer unsafe requests. Our findings\\nunderscore the fragile state of current model alignment and promote the\\nimportance of developing more robust alignment methods. Code and data will be\\nmade available at https://github.com/Linlt-leon/Adversarial-Alignments.', comment='8 pages, 7 figures', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.CL', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2407.03234v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2407.03234v1', title='pdf', rel='related', content_type=None)])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(search.results())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
