# arXiv CS.CL Newsletter for 2024-07-09

| Title | Authors | Brief Summary | Potential Applications | Link |
|-------|---------|---------------|------------------------|------|
| Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs | Rudolf Laine, Bilal Chughtai, Jan Betley, Kaivalya Hariharan, Jeremy Scheurer, Mikita Balesni, Marius Hobbhahn, Alexander Meinke, Owain Evans | The Situational Awareness Dataset (SAD) is a benchmark to assess the situational awareness in Large Language Models (LLMs), which refers to their knowledge of themselves and their current circumstances. | Understanding the situational awareness in LLMs can enhance their capacity for autonomous planning and action, but it also introduces novel risks related to AI safety and control, making it crucial for both automation and AI safety. | [Link](http://arxiv.org/pdf/2407.04694v1) |
| Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge | Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip Torr, Lu Yuan | The paper proposes a new visual prompt approach to integrate fine-grained external knowledge into multimodal large language models (MLLMs) for enhanced visual understanding. | This research has potential applications in various areas, such as image-text retrieval, visual question answering, and multimodal dialogue systems, where understanding detailed or localized visual elements is crucial. | [Link](http://arxiv.org/pdf/2407.04681v1) |
| Pretraining End-to-End Keyword Search with Automatically Discovered Acoustic Units | Bolaji Yusuf, Jan "Honza" Černocký, Murat Saraçlar | Pretraining end-to-end keyword search systems with untranscribed data using acoustic unit discovery for performance improvements. | Potential applications include improving the accuracy of keyword search systems for a wider range of languages and speech styles, as well as simplifying the keyword search pipeline by eliminating the need for automatic speech recognition. | [Link](http://arxiv.org/pdf/2407.04652v1) |
| Speculative Speech Recognition by Audio-Prefixed Low-Rank Adaptation of Language Models | Bolaji Yusuf, Murali Karthick Baskar, Andrew Rosenberg, Bhuvana Ramabhadran | Empowering conventional ASR with speculation capabilities to run ahead of audio and transcribe ongoing speech in real-time. | Improve speech recognition in real-time scenarios, such as voice assistants, speech-to-text software, and video conferencing applications. | [Link](http://arxiv.org/pdf/2407.04641v1) |
| Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework | Reza Averly, Xia Ning | A novel framework, entity decomposition with filtering, is introduced to improve clinical named entity recognition (NER) using open large language models (LLMs) for entity recognition. | The proposed framework can be applied to various clinical and healthcare contexts, such as enhancing patient records, improving clinical decision-making, and supporting research in medical domains. | [Link](http://arxiv.org/pdf/2407.04629v1) |
| Learning to (Learn at Test Time): RNNs with Expressive Hidden States | Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin | A new class of sequence modeling layers with linear complexity and expressive hidden states, where the hidden state is a machine learning model itself, updated by self-supervised learning at test time. | Natural Language Processing, Machine Translation, Speech Recognition, etc. | [Link](http://arxiv.org/pdf/2407.04620v1) |
| Written Term Detection Improves Spoken Term Detection | Bolaji Yusuf, Murat Saraçlar | A research paper proposes a multitask training objective to integrate unpaired text into end-to-end keyword search systems, improving search performance across various languages. | Domain adaptation in settings where in-domain paired data is scarce or nonexistent, also potentially improving document representations for words in unpaired text. | [Link](http://arxiv.org/pdf/2407.04601v1) |
| VRSD: Rethinking Similarity and Diversity for Retrieval in Large Language Models | Hang Gao, Yongfeng Zhang | A novel approach to characterizing similarity and diversity constraints in vector retrieval algorithms for Large Language Models (LLMs) through the relationship between the sum vector and the query vector. | This study can lead to significant enhancements in the capabilities of LLM-based agents in retrieving vectors that meet criteria for both similarity and diversity, potentially offering breakthroughs in various applications such as text summarization, information retrieval, and natural language processing. | [Link](http://arxiv.org/pdf/2407.04573v1) |
| Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition | Aditya K Surikuchi, Raquel Fernández, Sandro Pezzelle | Measuring the quality of visual storytelling requires evaluating human likeness in terms of visual grounding, coherence, and repetition, and not just relying on automatic metrics. | The novel method introduced in this paper can be applied to evaluate and improve the performance of various visual storytelling models, potentially leading to more competitive and human-like story generation. | [Link](http://arxiv.org/pdf/2407.04559v1) |
| Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations | Matthias Lindemann, Alexander Koller, Ivan Titov | This paper proposes intermediate pre-training of a Transformer to perform syntactic transformations of dependency trees, aiming to strengthen the structural inductive bias for seq2seq tasks. | The proposed method can improve few-shot learning of syntactic tasks and structural generalization for semantic parsing, potentially benefiting applications such as language translation, text summarization, and grammar checking tools. | [Link](http://arxiv.org/pdf/2407.04543v1) |
| PoPreRo: A New Dataset for Popularity Prediction of Romanian Reddit Posts | Ana-Cristina Rogoz, Maria Ilinca Nechita, Radu Tudor Ionescu | PoPreRo is the first dataset for predicting the popularity of Romanian Reddit posts, aiming to evaluate models on predicting the popularity of social media posts in Romanian. | PoPreRo can be used to improve the development of personalization algorithms, content recommendation systems, and social media analytics tools, particularly in the Romanian language. | [Link](http://arxiv.org/pdf/2407.04541v1) |
| From 'Showgirls' to 'Performers': Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs | Marion Bartl, Susan Leavy | A research paper that proposes adapting linguistic structures in Large Language Models (LLMs) training data to promote gender-inclusivity, reducing gender bias and stereotypes. | Potential applications include enhancing gender inclusivity in NLP, bias mitigation research, and language models used for tasks such as text summarization, language translation, and sentiment analysis. | [Link](http://arxiv.org/pdf/2407.04434v1) |
| ELCC: the Emergent Language Corpus Collection | Brendon Boldt, David Mortensen | The Emergent Language Corpus Collection (ELCC) is a collection of annotated corpora from open source implementations of emergent communication systems, enabling research to focus on the properties of emergent languages. | The availability of ELCC will enable new directions of research on properties of emergent languages themselves, rather than experimental apparatus, and potentially lead to advancements in understanding language emergence and development. | [Link](http://arxiv.org/pdf/2407.04158v1) |
| Stephanie: Step-by-Step Dialogues for Mimicking Human Interactions in Social Conversations | Hao Yang, Hongyuan Lu, Xinhua Zeng, Yang Liu, Xiang Zhang, Haoran Yang, Yumeng Zhang, Yiran Wei, Wai Lam | A novel Step-by-Step Dialogue Paradigm (Stephanie) is introduced to mimic human conversations by generating a high-quality step-by-step dialogue dataset to fine-tune existing large language models. | Stephanie has the potential to revolutionize chatbot technology and lead to more natural and effective human-computer interactions. | [Link](http://arxiv.org/pdf/2407.04093v1) |
| LLM Roleplay: Simulating Human-Chatbot Interaction | Hovhannes Tamoyan, Hendrik Schuff, Iryna Gurevych | LLM Roleplay is a method for automatically generating diverse multi-turn dialogues simulating human-chatbot interaction using large language models. | The method can be applied to create more realistic chatbots for various industries, such as customer service, healthcare, and entertainment, by generating dialogues with diverse sociodemographic backgrounds and conversational goals. | [Link](http://arxiv.org/pdf/2407.03974v1) |
| LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs | LLM-jp, :, Akiko Aizawa, Eiji Aramaki, Bowen Chen, Fei Cheng, Hiroyuki Deguchi, Rintaro Enomoto, Kazuki Fujii, Kensuke Fukumoto, Takuya Fukushima, Namgi Han, Yuto Harada, Chikara Hashimoto, Tatsuya Hiraoka, Shohei Hisada, Sosuke Hosokawa, Lu Jie, Keisuke Kamata, Teruhito Kanazawa, Hiroki Kanezashi, Hiroshi Kataoka, Satoru Katsumata, Daisuke Kawahara, Seiya Kawano, Atsushi Keyaki, Keisuke Kiryu, Hirokazu Kiyomaru, Takashi Kodama, Takahiro Kubo, Yohei Kuga, Ryoma Kumon, Shuhei Kurita, Sadao Kurohashi, Conglong Li, Taiki Maekawa, Hiroshi Matsuda, Yusuke Miyao, Kentaro Mizuki, Sakae Mizuki, Yugo Murawaki, Ryo Nakamura, Taishi Nakamura, Kouta Nakayama, Tomoka Nakazato, Takuro Niitsuma, Jiro Nishitoba, Yusuke Oda, Hayato Ogawa, Takumi Okamoto, Naoaki Okazaki, Yohei Oseki, Shintaro Ozaki, Koki Ryu, Rafal Rzepka, Keisuke Sakaguchi, Shota Sasaki, Satoshi Sekine, Kohei Suda, Saku Sugawara, Issa Sugiura, Hiroaki Sugiyama, Hisami Suzuki, Jun Suzuki, Toyotaro Suzumura, Kensuke Tachibana, Yu Takagi, Kyosuke Takami, Koichi Takeda, Masashi Takeshita, Masahiro Tanaka, Kenjiro Taura, Arseny Tolmachev, Nobuhiro Ueda, Zhen Wan, Shuntaro Yada, Sakiko Yahata, Yuya Yamamoto, Yusuke Yamauchi, Hitomi Yanaka, Rio Yokota, Koichiro Yoshino | A cross-organizational project for the research and development of open Japanese large language models (LLMs). | Real-world applications of the developed Japanese LLMs, including natural language processing, text generation, and machine translation, can have significant impacts on various industries and sectors such as education, healthcare, and finance. | [Link](http://arxiv.org/pdf/2407.03963v1) |
| Stark: Social Long-Term Multi-Modal Conversation with Persona Commonsense Knowledge | Young-Jun Lee, Dokyong Lee, Junyoung Youn, Kyeongjin Oh, Byungsoo Ko, Jonghwan Hyeon, Ho-Jin Choi | Stark is a large-scale long-term multi-modal conversation dataset that captures various social personas in a multi-modality format, bridging the gap between short-term image-sharing and long-term social interaction. | Potential applications may include developing more advanced chatbots and virtual assistants that can engage in long-term conversations and understand personalized image-sharing behavior. | [Link](http://arxiv.org/pdf/2407.03958v1) |
| Solving Zebra Puzzles Using Constraint-Guided Multi-Agent Systems | Shmuel Berman, Baishakhi Ray, Kathleen McKeown | A multi-agent system, ZPS, is introduced to solve zebra puzzles by integrating Large Language Models with an off-the-shelf theorem prover and a grid puzzle grader. | Potential applications include developing AI systems that can efficiently solve complex logical problems, improving natural language processing capabilities, and enhancing puzzle-solving abilities for various industries and domains. | [Link](http://arxiv.org/pdf/2407.03956v1) |
| Meta-prompting Optimized Retrieval-augmented Generation | João Rodrigues, António Branco | Meta-prompting optimization is introduced to refine retrieved content before it is included in the prompt, improving retrieval-augmented generation. | Multi-hop question answering task, other downstream tasks that leverage large language models | [Link](http://arxiv.org/pdf/2407.03955v1) |
| A framework for annotating and modelling intentions behind metaphor use | Gianluca Michelli, Xiaoyu Tong, Ekaterina Shutova | The paper proposes a novel taxonomy of intentions behind metaphor use, comprising 9 categories, and tests the capability of large language models in inferring intentions in zero- and few-shot settings. | This research can improve the natural language processing abilities of language models, enhancing their capabilities for tasks such as text analysis, summarization, and language translation. | [Link](http://arxiv.org/pdf/2407.03952v1) |
| Narrow Transformer: Starcoder-Based Java-LM For Desktop | Kamalkumar Rathinasamy, Balaji A J, Ankush Kumar, Gagan Gayari, Harshini K, Rajab Ali Mondal, Sreenivasa Raghavan K S, Swayam Singh | NT-Java-1.1B, a small Java code model, is developed to improve proficiency in Java programming and achieve comparable performance to larger models for desktop deployment. | Development of specialized code models for other programming languages, deployment of small code models on developer desktops, foundation for a family of NT Models across languages and sizes. | [Link](http://arxiv.org/pdf/2407.03941v1) |
| TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models | Jiahuan Cao, Dezhi Peng, Peirong Zhang, Yongxin Shi, Yang Liu, Kai Ding, Lianwen Jin | The concept of TongGu, a knowledge-grounded large language model, introduced in the paper, aims to improve the understanding of classical Chinese by leveraging a combination of instruction-tuning datasets, redundancy-aware tuning, and retrieval-augmented generation techniques. | The TongGu model and its accompanying dataset have potential applications in the fields of classical Chinese language learning, cultural heritage preservation, and historical research, enabling modern people to access and understand classical Chinese literature and texts more effectively. | [Link](http://arxiv.org/pdf/2407.03937v1) |
| Entity-Level Sentiment: More than the Sum of Its Parts | Egil Rønningstad, Roman Klinger, Erik Velldal, Lilja Øvrelid | The paper explores the complexity of sentiment analysis in longer texts, focusing on how sentiment is expressed towards individual entities and how these sentiments can be modeled. | The study and its dataset could potentially be used to improve sentiment analysis in various domains, such as social media monitoring, customer feedback analysis, and opinion mining. | [Link](http://arxiv.org/pdf/2407.03916v1) |
| Scoping Review of Active Learning Strategies and their Evaluation Environments for Entity Recognition Tasks | Philipp Kohl, Yoka Krämer, Claudia Fohry, Bodo Kraft | A scoping review of active learning strategies in natural language processing for entity recognition tasks, identifying 106 strategies and their evaluation environments. | The study's findings could be applied in real-world scenarios, such as information extraction from large datasets, to help researchers and practitioners make data-driven decisions about which active learning strategy to adopt. | [Link](http://arxiv.org/pdf/2407.03895v1) |
| DART: Deep Adversarial Automated Red Teaming for LLM Safety | Bojian Jiang, Yi Jing, Tianhao Shen, Qing Yang, Deyi Xiong | A novel automated red teaming framework, DART, that utilizes deep and dynamic interaction between the red team LLM and the target LLM to identify safety vulnerabilities in large language models. | Improved safety evaluation and mitigation of large language models in various applications such as natural language processing, language translation, and chatbots. | [Link](http://arxiv.org/pdf/2407.03876v1) |
| TartuNLP @ AXOLOTL-24: Leveraging Classifier Output for New Sense Detection in Lexical Semantics | Aleksei Dorkin, Kairit Sirts | A novel sense detection method that leverages classifier output to identify new senses and produce definitions for them in lexical semantics. | The proposed method can be applied to various natural language processing tasks, such as text classification, language modeling, and dialog systems, to improve their robustness to lexical semantics changes over time. | [Link](http://arxiv.org/pdf/2407.03861v1) |
| Anthropocentric bias and the possibility of artificial cognition | Raphaël Millière, Charles Rathkopf | Evaluating large language models' cognitive capacities requires overcoming anthropomorphic and anthropocentric biases. | This study's findings can have potential applications in developing more accurate and effective language processing systems, and improving the understanding of artificial cognition. | [Link](http://arxiv.org/pdf/2407.03859v1) |
| HYBRINFOX at CheckThat! 2024 -- Task 1: Enhancing Language Models with Structured Information for Check-Worthiness Estimation | Géraud Faye, Morgane Casanova, Benjamin Icard, Julien Chanson, Guillaume Gadek, Guillaume Gravier, Paul Égré | Enhancing language models with structured information from triples for check-worthiness estimation | Improving fact-checking capabilities of language models, especially in detecting false information on the web, social media, and news outlets | [Link](http://arxiv.org/pdf/2407.03850v1) |
| On the Benchmarking of LLMs for Open-Domain Dialogue Evaluation | John Mendonça, Alon Lavie, Isabel Trancoso | The paper discusses the limitations of current evaluation benchmarks for assessing the capabilities of state-of-the-art chatbot models. | Potential applications include the development of more accurate evaluation benchmarks, improving the performance of chatbot models, and enhancing the overall quality of human-computer interactions. | [Link](http://arxiv.org/pdf/2407.03841v1) |
| Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation | Tiia Sildam, Andra Velve, Tanel Alumäe | This paper investigates the finetuning of end-to-end models for bidirectional Estonian-English and Estonian-Russian conversational speech-to-text translation using synthetic data. | The study's findings and techniques can be applied to improve speech-to-text translation systems for low-resource languages, and enhance the development of conversational AI applications for Estonian and other similar languages. | [Link](http://arxiv.org/pdf/2407.03809v1) |
| Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning | Thong Nguyen, Yi Bin, Xiaobao Wu, Xinshuai Dong, Zhiyuan Hu, Khoi Le, Cong-Duy Nguyen, See-Kiong Ng, Luu Anh Tuan | A meta-optimized framework for video-language representation learning that addresses issues of misaligned video-text pairs and uneven concept distribution. | Improved video question answering and text-video retrieval capabilities, with potential applications in multimedia search, video summarization, and language-based video understanding. | [Link](http://arxiv.org/pdf/2407.03788v1) |
| HYBRINFOX at CheckThat! 2024 -- Task 2: Enriching BERT Models with the Expert System VAGO for Subjectivity Detection | Morgane Casanova, Julien Chanson, Benjamin Icard, Géraud Faye, Guillaume Gadek, Guillaume Gravier, Paul Égré | The HYBRINFOX method combines a RoBERTa model, sBERT model, and expert system VAGO to solve Task 2 of Subjectivity detection in the CLEF 2024 CheckThat! competition, achieving the 1st rank with a macro F1 score of 0.7442 on the evaluation data in English. | The method could be improved for other languages besides English, with potential applications in various NLP tasks, including sentiment analysis and opinion mining. | [Link](http://arxiv.org/pdf/2407.03770v1) |
| Multi-Convformer: Extending Conformer with Multiple Convolution Kernels | Darshan Prabhu, Yifan Peng, Preethi Jyothi, Shinji Watanabe | The Multi-Convformer model uses multiple convolution kernels with gating to improve the modeling of local dependencies at varying granularities in end-to-end Automatic Speech Recognition systems. | Potential applications of the Multi-Convformer model include developing more efficient and effective Automatic Speech Recognition systems, as well as improving speech recognition capabilities in various settings such as voice assistants, smart home devices, and telemedicine platforms. | [Link](http://arxiv.org/pdf/2407.03718v1) |
| Contrastive Chain-of-Thought Prompting | Grant Kruttschnitt, Jay Shim, Alyssa Ma, Daniel Kim, Benjamin Chek, Athul Anand, Kevin Zhu, Sean O'Brien | A method that leverages chain-of-thought prompting and context-aware decoding to improve language model reasoning, and introduces input-based contrasting methods to encourage contextual understanding. | The study's findings have potential applications in developing more accurate and context-aware language models, which can be used in various domains such as natural language processing, text classification, and question answering. | [Link](http://arxiv.org/pdf/2407.03600v1) |
| Integrating Randomness in Large Language Models: A Linear Congruential Generator Approach for Generating Clinically Relevant Content | Andrew Bouras | A Linear Congruential Generator method is used to generate diverse, high-quality outputs from language models by selecting unique combinations of clinical facts and integrating them into prompts. | Education and content creation applications in which diverse, high-quality outputs from language models are crucial, such as generating clinically relevant content for patient education, creating training materials for healthcare professionals, or producing medical information articles. | [Link](http://arxiv.org/pdf/2407.03582v1) |
| STF: Sentence Transformer Fine-Tuning For Topic Categorization With Limited Data | Kheir Eddine Daouadi, Yaakoub Boualleg, Oussama Guehairia | Sentence Transformer Fine-Tuning (STF) is a topic detection system that leverages pre-trained Sentence Transformers models and fine-tuning to classify tweet topics accurately. | Tweet sentiment analysis, automated topic modeling, text classification, and social media monitoring, which can be applied to various industries such as marketing, customer service, and research. | [Link](http://arxiv.org/pdf/2407.03253v1) |
